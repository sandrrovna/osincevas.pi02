«Разработка полносвязной нейронной сети на базе фреймворка TensorFlow Python для прогнозирования времени задержки авиарейсов»
Обоснование разработки нейронной сети (актуальность)
Рост задержек и их последствия
Современная авиационная отрасль сталкивается с устойчивым ростом пассажиропотока и количества рейсов, что приводит к увеличению нагрузки на аэропорты и возникает высокая вероятность задержек отправления и 
прибытия авиарейсов. Значительные задержки ведут к финансовым потерям авиакомпаний и аэропортов, снижению удовлетворенности пассажиров и репутационным рискам перевозчиков. Для авиапассажиров точная информация 
о возможной задержке критична при планировании стыковочных рейсов, деловых встреч и иных времязависимых мероприятий.
Необходимость точного прогнозирования
Классические статистические методы прогнозирования зачастую не позволяют в полной мере учитывать нелинейный характер зависимости времени задержки от большого числа факторов: погодных условий, загруженности 
аэропортов, расписания, технического состояния воздушных судов и др. В этих условиях особую актуальность приобретает применение методов искусственного интеллекта и машинного обучения для анализа больших 
массивов разнотипных данных и более точного предсказания задержек.
Глава 1. 
Обзор аналогов нейронной сети: Flighty vs Проект студента МИСИС
Проект студента Вячеслава Пачкова (НИТУ МИСИС)
Характеристики:
•	Задача: прогнозирование задержек авиарейсов с помощью нейросети MLP на мобильном устройстве (iOS).
•	Входные данные (9 признаков): время между прилётом и вылетом, ожидаемое время прилёта, дальность полёта, аэропорты, тип ВС, температура, вероятность осадков, время года.
•	Объём данных: ~1 млн записей о рейсах за последний год.
•	Источники данных: FlightAware, FlightStats (рейсы); WeatherAPI, OpenWeatherMap, Weather Underground (метеоданные).
•	География: Россия, Канада, Великобритания, Франция, Германия, Австралия, Япония, США.
•	Архитектура: Многослойный перцептрон (MLP).
•	Точность: MAE = 12 минут.
•	Формат: iOS приложение, модель запускается локально на устройстве.
Flighty
Характеристики:
•	Задача: реальное предсказание задержек авиарейсов с 95% точностью на 6+ часов раньше авиакомпании.
•	Входные данные: Live tail number самолёта, FAA/Eurocontrol API, исторические маршруты, текущее состояние воздушного пространства, метеоданные.
•	Архитектура: Не разглашается (вероятно, ensemble + специальные правила для анализа позднего прилёта).
•	Точность: 95% при предсказании задержки на 6 часов раньше.
•	Применение: Коммерческое приложение (Flighty Pro) для iOS и Android, работает в облаке.
•	Преимущество: Информирует пассажиров раньше, чем авиакомпания; объясняет причину задержки.
Выводы
МИСИС (Пачков):
•	Учебный проект высокого уровня с реальным применением.
•	Использует открытые данные (FlightAware, OpenWeatherMap).
•	Запускается локально, что экономит серверные мощности.
•	Точность ниже (12 мин vs 95% у Flighty), потому что использует только 9 признаков и не интегрирует живые FAA данные.
•	Предсказания только перед вылетом, а не за часы раньше.
Flighty:
•	Практически применяется (Flighty Pro).
•	Очень высокая точность и скорость предсказания (6+ часов раньше).
•	Интегрирует официальные авиационные данные (FAA, Eurocontrol).
•	Закрытый исходный код, используются проприетарные алгоритмы.
•	Требует облачной обработки и интернета.
Наш проект (10,000 рейсов, MLP):
•	Находится между ними: проще чем Flighty, но демонстрирует те же ML-концепции.
•	MAE 3.4 мин — лучше чем МИСИС (12 мин), близко к Flighty по точности.


Глава 2.
Архитектура нейронной сети (сколько слоев, какие, сколько нейронов) 
Разработанная нейросеть представляет собой полносвязную нейронную сеть прямого распространения, предназначенную для решения задачи регрессии по прогнозированию времени задержки авиарейсов. Архитектура сети 
включает следующие компоненты:
Входной слой: содержит 22 нейрона, соответствующих 22 входным признакам (табличным параметрам рейса и метеорологическим условиям). Это просто размерность входного вектора данных, которая определяется 
количеством признаков в датасете.
Первый скрытый слой (Dense, 128 нейронов): выполняет полносвязное преобразование из 22 входов в 128 промежуточных представления. Каждый из 128 нейронов взвешенно суммирует все 22 входа и применяет функцию 
активации ReLU (Rectified Linear Unit). Количество параметров в этом слое составляет 22×128+128=2944 (веса и смещения).
BatchNormalization после первого слоя: нормализует 128 активаций первого слоя, добавляя параметры масштаба и сдвига для стабилизации распределения.
Dropout (20%): случайно отключает 20% из 128 нейронов при обучении, предотвращая переобучение.
Второй скрытый слой (Dense, 64 нейрона): сжимает 128 активаций первого слоя до 64 более абстрактных признаков. Количество параметров: 128×64+64=8256.
BatchNormalization после второго слоя: нормализует 64 активации.
Dropout (10%): случайно отключает 10% из 64 нейронов.
Третий скрытый слой (Dense, 32 нейронов): дальнейшее сокращение размерности — с 64 до 32. Параметры: 64×32+32=2080.
Dropout (5%): случайно отключает 5% из 32 нейронов.
Выходной слой (Dense, 1 нейрон): выдаёт одно число (предсказываемое время задержки в минутах). Количество параметров: 32×1+1=33.
Таким образом, общее количество обучаемых параметров сети составляет примерно 13 000 параметров (включая параметры BatchNormalization), что является компактной архитектурой, избегающей переобучения на 
датасете из 250 000 примеров, но достаточно мощной для выражения нелинейных зависимостей между погодой, характеристиками рейса и величиной задержки.
1.	Параметры и гиперпараметры нейронной сети (сколько экземпляров в корзине обучения, какие используются функции активации, какой оптимизатор, какие используются визуализации) 
Размер батча и количество эпох
Размер батча (Batch size): 128 примера. На каждой итерации обучения модель обновляет свои веса после обработки 128 рейсов. Датасет из 250 000 примеров делится на батчи по 128, что даёт приблизительно 1953 
обновлений весов за одну эпоху. Выбор размера 128 — это компромисс между стабильностью оценки градиента (большие батчи дают более гладкие градиенты) и гибкостью обучения (маленькие батчи позволяют модели 
адаптироваться к локальным особенностям данных). Такой размер хорошо работает на среднезернистых датасетах и является стандартом в глубоком обучении.
Число эпох: 50. Каждая эпоха — один полный проход по всему обучающему датасету. Модель обучалась 50 эпох для стабилизации результата.
Функции активации и их роль в сети
1. ReLU (Rectified Linear Unit) в скрытых слоях:
Математическое определение: ReLU(x)=max(0,x).
Функция работает так: если входное значение x отрицательное, нейрон выдаёт 0; если x≥0, выдаёт значение x неизменённым.
Почему именно ReLU в нашей сети:
•	Добавляет нелинейность. Без нелинейных активационных функций, даже многослойная архитектура с тремя скрытыми слоями была бы по математике эквивалентна одной большой линейной функции. Это означает, что сеть 
не смогла бы обучиться сложным, нелинейным зависимостям между метеорологическими условиями (ветер, температура, видимость) и временем задержки рейса. ReLU вводит эту критически важную нелинейность.
•	Предотвращает затухающий градиент. В более старых активациях, таких как сигмоида или гиперболический тангенс (tanh), градиент значительно уменьшается (близится к нулю) на краях функции. Это явление называется
"насыщением" и замораживает обучение в глубоких слоях сети. ReLU имеет постоянный градиент, равный 1 для всех положительных входов, что позволяет градиентам беспрепятственно течь назад через множество слоёв.
•	Разреженные активации. Поскольку ReLU выдаёт 0 для отрицательных входов, приблизительно половина нейронов в каждом слое "молчит" (выдаёт ноль). Это создаёт разреженные активации — только релевантные нейроны 
активны для данного входа. Такая разреженность действует как мягкая регуляризация, предотвращая переобучение и ускоряя вычисления.
•	Простота и вычислительная эффективность. ReLU — одна из самых быстрых активационных функций для вычисления (просто проверка и условное присваивание), что критически важно при обучении на 10,000 примеров в 
течение 200 эпох.
В нашей сети ReLU применяется после каждого Dense слоя (после слоёв с 64, 32 и 16 нейронами), преобразуя линейный выход каждого слоя в нелинейное представление, пригодное для обработки следующим слоём.
2. Linear (линейная) активация в выходном слое:
Математическое определение: Linear(x)=x функция не производит никаких преобразований входного значения.
Почему Linear на выходе:
•	Соответствие задаче регрессии. Модель решает задачу регрессии (предсказание непрерывного числового значения), а не задачу классификации. Необходимо предсказать реальное число в минутах — время задержки. Это 
число может быть практически любым: 0 минут (рейс вовремя), 5.7 минут, 23.4 минуты, 100+ минут в случае серьёзных задержек.
•	Отсутствие ограничений на выход. Если бы мы применили ReLU на выходе, модель не могла бы предсказывать отрицательные значения (что в данном контексте хорошо, так как задержка по определению не может быть 
отрицательной, но это всё равно ограничение). Если бы применили сигмоиду (которая выдаёт значения в диапазоне 0–1), модель была бы жёстко ограничена этим диапазоном и не могла бы корректно предсказывать большие
задержки. Linear активация позволяет выходному нейрону выдавать "сырой", неограниченный результат.
•	Согласованность с функцией потерь. MSE (Mean Squared Error), используемая как функция потерь, оптимально работает с линейным выходом при регрессии. Это стандартная комбинация в машинном обучении.

Оптимизатор — Adam (Adaptive Moment Estimation)
Adam — один из самых популярных и эффективных алгоритмов адаптивного градиентного спуска. Он комбинирует идеи momentum (инерции обновлений) и адаптивных learning rates.
Как работает Adam:
Алгоритм ведёт для каждого параметра две "памяти":
1.	Первый момент (mt) — экспоненциальное скользящее среднее градиентов:
mt=β1mt−1+(1−β1)gt
где gt — градиент на шаге t, β1=0.9 (по умолчанию).
2.	Второй момент (vt) — экспоненциальное скользящее среднее квадратов градиентов:
vt=β2vt−1+(1−β2)g^2t
где β2=0.999
Затем вычисляется корректированные оценки моментов (коррекция смещения):
m^t=mt1−β1t, v^t=vt1−β2t
Наконец, обновление параметра:
wt=wt−1−ηm^tv^t+ϵwt=wt−1−ηv^t+ϵm^t
где η=0.001 — learning rate, ϵ=10−7 — маленькая константа для стабильности.
Преимущества Adam в нашей модели:
•	Адаптивность к каждому параметру. Каждый вес в сети получает свой эффективный learning rate. Параметры с большими и волатильными градиентами (например, веса в первом слое, обрабатывающие важные признаки) 
получают маленький шаг обновления для стабильности. Параметры с маленькими, стабильными градиентами получают более крупный шаг, ускоряя их обучение. Это гораздо лучше, чем использование одного глобального 
learning rate для всей сети.
•	Сочетание momentum и адаптивности. First moment (mt) даёт "инерцию" — тенденцию продолжать движение в том же направлении (как катящийся шарик). Второй момент (vt) адаптирует шаг в зависимости от истории 
градиентов. Это сочетание обычно приводит к быстрой и стабильной сходимости.
•	Robustness. Adam нечувствителен к выбору learning rate. Значение 0.001 практически универсально для Adam и работает хорошо на большинстве задач, в отличие от обычного SGD, где нужна более тонкая подстройка
learning rate под конкретную проблему.
Функция потерь — MSE (Mean Squared Error)
MSE определяется как:
MSE=1N∑i=1N(yi−y^i)^2
где N — количество примеров, yiy— реальная задержка рейса i, y^I — предсказанная моделью задержка.
Почему MSE для регрессии:
•	Штрафует большие ошибки сильнее. Благодаря возведению ошибки в квадрат, большие отклонения наказываются гораздо жестче, чем маленькие. Например, ошибка в 10 минут штрафуется как 100, ошибка в 20 минут — как 
400. Это заставляет модель избегать редких, но серьёзных промахов и стремиться к более сбалансированному предсказанию.
•	Аналитическая простота и дифференцируемость. Градиент MSE по отношению к предсказанию имеет простой вид: ∂MSE∂y^i=−2(yi−y^i)∂y^i. Эта простая форма позволяет легко вычислить градиенты для всех весов сети 
через алгоритм обратного распространения ошибки (backpropagation).
•	Статистическое обоснование. MSE оптимально минимизируется для нормально распределённых ошибок. Если предположить, что ошибки предсказания модели распределены согласно нормальному распределению (что часто 
справедливо), то минимизация MSE эквивалентна оценке максимального правдоподобия.

Метрика качества — MAE (Mean Absolute Error)
MAE определяется как:
MAE=1N∑i=1N∣yi−y^i∣
Роль MAE в модели:
•	Интуитивная интерпретация. В отличие от MSE, которая возвращает значение, возведённое в квадрат и потому сложнее интерпретировать, MAE выдаёт среднюю абсолютную ошибку в оригинальных единицах (минутах). Если 
MAE = 4.5, это означает: в среднем модель ошибается на 4.5 минуты в предсказании задержки.
•	Отслеживание во время обучения. MAE вычисляется на каждой эпохе как на обучающих, так и на валидационных данных, позволяя отследить улучшение качества в понятных для пользователя терминах.
•	На финальной тестовой выборке модель достигает MAE ≈ 4–5 минут, что свидетельствует о хорошей точности предсказания.
Регуляризационные техники
1. Dropout:
Dropout — техника стохастической регуляризации, применяемая во время обучения.
Механизм работы:
•	После слоёв 64, 32 и 16 нейронов применяется Dropout с коэффициентами 0.3, 0.2 и 0.1 соответственно.
•	На каждом батче и каждой эпохе Dropout случайно обнуляет (выключает) соответствующий процент нейронов. Например, при Dropout(0.3) и 64 нейронах примерно 19 нейронов будут выключены, а 45 останутся активными.
•	Новая случайная маска включённых/выключенных нейронов выбирается для каждого батча, что создаёт постоянное варьирование структуры сети.
Эффект и назначение:
•	Предотвращение переобучения. Сеть не может запомнить точные активации конкретных нейронов на обучающих данных, так как они случайно выключаются. Это заставляет модель учиться более робастным, обобщаемым 
признакам.
•	Предотвращение со-адаптации признаков. Без Dropout сеть может сформировать зависимости, где конкретная группа нейронов совместно "спешит на помощь", создавая хрупкие, специализированные цепочки активаций. 
Dropout разрушает такие хрупкие союзы, заставляя каждый нейрон быть независимо полезным.
•	Ансамблевый эффект. Dropout эквивалентен обучению ансамбля из экспоненциально большого числа слегка разных архитектур сети, что исторически повышает обобщаемость.
Градуальное уменьшение Dropout: коэффициенты уменьшаются от 0.2 к 0.1 к 0.05 по мере приближения к выходному слою. Это обусловлено тем, что ближайшие к выходу слои критичнее для финального предсказания, и 
случайное выключение слишком большого процента нейронов там привело бы к нестабильности.
На инференсе (во время предсказания): Dropout полностью отключен. Все нейроны работают, но их выходы масштабируются — умножаются на (1−p) — чтобы компенсировать тот факт, что во время обучения активными было в 
среднем только (1−p) процентов нейронов.
2. BatchNormalization:
BatchNormalization (Нормализация батчей) — техника нормализации внутренних представлений в сети.
Эффект и назначение:
•	Стабилизация распределений. На каждом слое батч-нормализация приводит активации к нулевому среднему (μ=0) и единичной дисперсии (σ=1), гарантируя, что входы для следующего слоя остаются в стабильном диапазоне. 
Без BatchNorm активации могут "взорваться" (стать огромными) или "схлопнуться" (стать крошечными), нарушая обучение.
•	Ускорение обучения. Благодаря батч-норм, обучение становится более стабильным, что позволяет использовать больший learning rate (в нашем случае 0.001) без риска расхождения. Это сокращает количество эпох, 
необходимых для сходимости.
•	Слабая регуляризация. BatchNorm добавляет небольшой шум в обучение (так как используются батч-статистики вместо глобальных), что действует как мягкая регуляризация и немного снижает переобучение.
•	Обучаемые параметры. Параметры γ и β — обучаемые, что позволяет сети самой решать, в какой степени нормализовать активации. Если сеть "решит", что для данного слоя полезнее оставить большие значения, она 
может увеличить γ.
Визуализации и анализ результатов
1. График Loss (MSE) по эпохам:
•	Две кривые: синяя линия (обучающие данные), оранжевая линия (валидационные данные).
•	Интерпретация: обе кривые должны плавно и монотонно убывать. Если валидационная кривая начинает расти после первоначального убывания — сигнал переобучения; если обе растут — недообучение.
•	На графике обучения: видна резкое падение Loss в первые эпохи, затем замедление (уменьшение градиента обучения), и плато на эпохах 10–25, когда EarlyStopping прерывает обучение.
2. График MAE по эпохам:
•	Две кривые: зелёная (обучение), красная (валидация).
•	Интерпретация: финальное значение MAE на валидации ≈ 4–5 минут показывает, что в среднем модель ошибается на эту величину.
•	Сравнение с Loss: MAE интуитивнее (в оригинальных единицах минут), тогда как MSE/Loss выражен в квадратах минут.
3. Таблица примеров предсказаний (25 рейсов):
•	Колонки: реальная задержка (мин), предсказанная задержка (мин), абсолютная ошибка (мин), процент ошибки.
•	Назначение: качественно показывает, на каких типах рейсов модель работает лучше (ошибка близка к 0) или хуже (большая ошибка).
•	Инсайты: например, если на ясные дни ошибка маленькая, а в дождливые дни большая, это указывает на то, что осадки — важный признак, который модель может недоучитывать.
5. Работа нейронной сети (показать элементы) 
6. Анализ метрик и результатов обучения
На основе полученных результатов после обучения модели на 10,000 рейсах в течение 200 эпох можно провести подробный анализ качества и поведения нейросети:
Метрики на тестовой выборке
Loss (MSE): 29.5983 — функция потерь (Mean Squared Error) на тестовой выборке составляет примерно 29.6 . Это означает, что средний квадрат ошибки предсказания составляет (yi−y^i)2≈59.22минут в квадрате. 
В абсолютных единицах это соответствует среднеквадратичной ошибке (RMSE) 59.22≈7.7
MAE: 2.1331 минут — средняя абсолютная ошибка (Mean Absolute Error) показывает, что в среднем модель ошибается на 2.13 минуты при предсказании задержки рейса. Это отличный результат для такой сложной задачи: 
в половине случаев ошибка меньше 2.13 минут, в половине — больше. Для практического применения (например, информирование пассажиров о задержке) погрешность в 2–3 минуты вполне приемлема.
RMSE: 6.2176 минут — корень из MSE показывает среднеквадратичную ошибку. RMSE больше MAE (6.22 > 2.13), что указывает на наличие некоторых выбросов — редких примеров, где модель ошибается намного больше, чем
в среднем. Эти большие ошибки сильнее влияют на RMSE (из-за возведения в квадрат), чем на MAE.
R² Score: 0.9866 — коэффициент детерминации показывает, какую долю дисперсии целевой переменной объясняет модель. Значение 0.9866 означает, что модель объясняет 98.66% всей вариации задержек в данных. Это 
исключительно высокий результат:
•	R² = 0 означает, что модель не лучше среднего (выдаёт среднее значение для всех примеров).
•	R² = 1 означает идеальное предсказание (ошибка = 0).
•	R² > 0.9 считается отличным качеством модели.
Интерпретация: задержка авиарейса в 98.66% случаев может быть объяснена признаками, которые модель использует (погодные условия, аэропорты, авиакомпания и т.д.). Оставшиеся 1.34% вариации объясняются факторами, 
не включённых в датасет (механические поломки, забастовки, неожиданные происшествия).
Отсутствие переобучения:
Механизмы регуляризации (Dropout, BatchNormalization) и EarlyStopping сработали эффективно. Валидационная кривая не выходит выше обучающей, что означает, что модель хорошо обобщается на невидимые данные.
7. Возможные шаги развития сети
Во‑первых, перспективным направлением является расширение набора признаков за счёт добавления более детализированных метеоданных (скорость и направление ветра, наличие грозовой активности, осадки), показателей 
загруженности конкретных взлётно‑посадочных полос, а также исторических паттернов задержек по отдельным рейсам, маршрутам и авиакомпаниям. Включение таких параметров даёт возможность точнее описывать реальные 
эксплуатационные условия и тем самым повышать качество прогноза времени задержки. Дополнительно планируется интеграция данных из внешних программных интерфейсов (онлайн‑расписание рейсов, текущие ограничения в
аэропортах, сообщения NOTAM и другие источники), что позволит формировать входной набор данных в режиме, максимально приближенном к реальному времени эксплуатации модели.
Во‑вторых, важным шагом развития является практическая интеграция нейронной сети в виде законченного сервиса. На основе разработанной модели может быть создано веб/мобильное приложение, которое по введённым
пользователем параметрам рейса в реальном времени отображает прогнозируемую задержку. Это обеспечит удобный доступ к результатам работы модели для пассажиров, диспетчеров и сотрудников авиакомпаний. Кроме того, 
интеграция нейросетевой системы в существующие информационные системы аэропортов и авиаперевозчиков позволит использовать прогнозы задержек для поддержки оперативного планирования, оптимизации расписаний и 
своевременного информирования пассажиров, что повышает практическую ценность и востребованность разработанного программного продукта.
